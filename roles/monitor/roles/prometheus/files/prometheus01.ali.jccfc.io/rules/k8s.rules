groups:
    - name: k8s.rules
      rules:
      - expr: |
          sum(rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container!="POD"}[5m])) by (namespace)
        record: namespace:container_cpu_usage_seconds_total:sum_rate
      - expr: |
          sum by (namespace, pod, container) (
            rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container!="POD"}[5m])
          )
        record: namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
      - expr: |
          sum(container_memory_usage_bytes{job="kubelet", image!="", container!="POD"}) by (namespace, idc, env)
        record: namespace:container_memory_usage_bytes:sum
      - expr: |
          sum by (namespace, label_name) (
              sum(rate(container_cpu_usage_seconds_total{job="kubelet", image!="", container!="POD"}[5m])) by (namespace, pod)
            * on (namespace, pod)
              group_left(label_name) kube_pod_labels{job="kube-state-metrics"}
          )
        record: namespace:container_cpu_usage_seconds_total:sum_rate
      - expr: |
          sum by (namespace, label_name, idc, env) (
              sum(container_memory_usage_bytes{job="kubelet",image!="", container!="POD"}) by (pod, namespace, idc, env)
            * on (namespace, pod)
              group_left(label_name) kube_pod_labels{job="kube-state-metrics"}
          )
        record: namespace:container_memory_usage_bytes:sum
      - expr: |
          sum by (namespace, label_name) (
              sum(kube_pod_container_resource_requests_memory_bytes{job="kube-state-metrics"} * on (endpoint, instance, job, namespace, pod, service) group_left(phase) (kube_pod_status_phase{phase=~"^(Pending|Running)$"} == 1)) by (namespace, pod)
            * on (namespace, pod)
              group_left(label_name) kube_pod_labels{job="kube-state-metrics"}
          )
        record: namespace:kube_pod_container_resource_requests_memory_bytes:sum
      - expr: |
          sum by (namespace, label_name) (
              sum(kube_pod_container_resource_requests_cpu_cores{job="kube-state-metrics"} * on (endpoint, instance, job, namespace, pod, service) group_left(phase) (kube_pod_status_phase{phase=~"^(Pending|Running)$"} == 1)) by (namespace, pod)
            * on (namespace, pod)
              group_left(label_name) kube_pod_labels{job="kube-state-metrics"}
          )
        record: namespace:kube_pod_container_resource_requests_cpu_cores:sum
    - name: kube-scheduler.rules
      rules:
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="kube-scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
    - name: node.rules
      rules:
      - expr: sum(min(kube_pod_info) by (node))
        record: ':kube_pod_info_node_count:'
      - expr: |
          max(label_replace(kube_pod_info{job="kube-state-metrics"}, "pod", "$1", "pod", "(.*)")) by (node, namespace, pod)
        record: 'node_namespace_pod:kube_pod_info:'
      - expr: |
          count by (node) (sum by (node, cpu) (
            node_cpu_seconds_total{job="monitoring/node_exporter"}
          * on (namespace, pod) group_left(node)
            node_namespace_pod:kube_pod_info:
          ))
        record: node:node_num_cpu:sum
      - expr: |
          1 - avg(rate(node_cpu_seconds_total{job="monitoring/node_exporter",mode="idle"}[1m]))
        record: :node_cpu_utilisation:avg1m
      - expr: |
          1 - avg by (node) (
            rate(node_cpu_seconds_total{job="monitoring/node_exporter",mode="idle"}[1m])
          * on (namespace, pod) group_left(node)
            node_namespace_pod:kube_pod_info:)
        record: node:node_cpu_utilisation:avg1m
      - expr: |
          node:node_cpu_utilisation:avg1m
            *
          node:node_num_cpu:sum
            /
          scalar(sum(node:node_num_cpu:sum))
        record: node:cluster_cpu_utilisation:ratio
      - expr: |
          sum(node_load1{job="monitoring/node_exporter"})
          /
          sum(node:node_num_cpu:sum)
        record: ':node_cpu_saturation_load1:'
      - expr: |
          sum by (node) (
            node_load1{job="monitoring/node_exporter"}
          * on (namespace, pod) group_left(node)
            node_namespace_pod:kube_pod_info:
          )
          /
          node:node_num_cpu:sum
        record: 'node:node_cpu_saturation_load1:'
      - expr: |
          1 -
          sum(node_memory_MemFree_bytes{job="monitoring/node_exporter", app="kube-node"} + node_memory_Cached_bytes{job="monitoring/node_exporter", app="kube-node"} + node_memory_Buffers_bytes{job="monitoring/node_exporter", app="kube-node"})
          /
          sum(node_memory_MemTotal_bytes{job="monitoring/node_exporter"})
        record: ':node_memory_utilisation:'
      - expr: |
          sum(node_memory_MemFree_bytes{job="monitoring/node_exporter", app="kube-node"} + node_memory_Cached_bytes{job="monitoring/node_exporter", app="kube-node"} + node_memory_Buffers_bytes{job="monitoring/node_exporter", app="kube-node"})
        record: :node_memory_MemFreeCachedBuffers_bytes:sum
      - expr: |
          sum(node_memory_MemTotal_bytes{job="monitoring/node_exporter"})
        record: :node_memory_MemTotal_bytes:sum
      - expr: |
          sum by (node) (
            (node_memory_MemFree_bytes{job="monitoring/node_exporter", app="kube-node"} + node_memory_Cached_bytes{job="monitoring/node_exporter", app="kube-node"} + node_memory_Buffers_bytes{job="monitoring/node_exporter", app="kube-node"})
            * on (namespace, pod) group_left(node)
              node_namespace_pod:kube_pod_info:
          )
        record: node:node_memory_bytes_available:sum
      - expr: |
          sum by (node) (
            node_memory_MemTotal_bytes{job="monitoring/node_exporter"}
            * on (namespace, pod) group_left(node)
              node_namespace_pod:kube_pod_info:
          )
        record: node:node_memory_bytes_total:sum
      - expr: |
          (node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)
          /
          node:node_memory_bytes_total:sum
        record: node:node_memory_utilisation:ratio
      - expr: |
          (node:node_memory_bytes_total:sum - node:node_memory_bytes_available:sum)
          /
          scalar(sum(node:node_memory_bytes_total:sum))
        record: node:cluster_memory_utilisation:ratio
      - expr: |
          1e3 * sum(
            (rate(node_vmstat_pgpgin{job="monitoring/node_exporter"}[1m])
           + rate(node_vmstat_pgpgout{job="monitoring/node_exporter"}[1m]))
          )
        record: :node_memory_swap_io_bytes:sum_rate
      - expr: |
          1 -
          sum by (node) (
            (node_memory_MemFree_bytes{job="monitoring/node_exporter", app="kube-node"} + node_memory_Cached_bytes{job="monitoring/node_exporter", app="kube-node"} + node_memory_Buffers_bytes{job="monitoring/node_exporter", app="kube-node"})
          * on (namespace, pod) group_left(node)
            node_namespace_pod:kube_pod_info:
          )
          /
          sum by (node) (
            node_memory_MemTotal_bytes{job="monitoring/node_exporter"}
          * on (namespace, pod) group_left(node)
            node_namespace_pod:kube_pod_info:
          )
        record: 'node:node_memory_utilisation:'
      - expr: |
          1 - (node:node_memory_bytes_available:sum / node:node_memory_bytes_total:sum)
        record: 'node:node_memory_utilisation_2:'
      - expr: |
          1e3 * sum by (node) (
            (rate(node_vmstat_pgpgin{job="monitoring/node_exporter"}[1m])
           + rate(node_vmstat_pgpgout{job="monitoring/node_exporter"}[1m]))
           * on (namespace, pod) group_left(node)
             node_namespace_pod:kube_pod_info:
          )
        record: node:node_memory_swap_io_bytes:sum_rate
      - expr: |
          avg(irate(node_disk_io_time_seconds_total{job="monitoring/node_exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m]))
        record: :node_disk_utilisation:avg_irate
      - expr: |
          avg by (node) (
            irate(node_disk_io_time_seconds_total{job="monitoring/node_exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m])
          * on (namespace, pod) group_left(node)
            node_namespace_pod:kube_pod_info:
          )
        record: node:node_disk_utilisation:avg_irate
      - expr: |
          avg(irate(node_disk_io_time_weighted_seconds_total{job="monitoring/node_exporter",device=~"nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+"}[1m]))
        record: :node_disk_saturation:avg_irate
      - expr: |
          max(
            max(
              kube_pod_info{job="kube-state-metrics", host_ip!=""}
            ) by (node, host_ip)
            * on (host_ip) group_right (node)
            label_replace(
              (max(node_filesystem_files{job="monitoring/node_exporter", mountpoint="/"}) by (instance)), "host_ip", "$1", "instance", "(.*):.*"
            )
          ) by (node)
        record: 'node:node_inodes_total:'
      - expr: |
          max(
            max(
              kube_pod_info{job="kube-state-metrics", host_ip!=""}
            ) by (node, host_ip)
            * on (host_ip) group_right (node)
            label_replace(
              (max(node_filesystem_files_free{job="monitoring/node_exporter", mountpoint="/"}) by (instance)), "host_ip", "$1", "instance", "(.*):.*"
            )
          ) by (node)
        record: 'node:node_inodes_free:'
    - name: kube-prometheus-node-recording.rules
      rules:
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[3m])) BY
          (instance)
        record: instance:node_cpu:rate:sum
      - expr: sum((node_filesystem_size_bytes{mountpoint="/"} - node_filesystem_free_bytes{mountpoint="/"}))
          BY (instance)
        record: instance:node_filesystem_usage:sum
      - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
        record: instance:node_network_receive_bytes:rate:sum
      - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
        record: instance:node_network_transmit_bytes:rate:sum
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m])) WITHOUT
          (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total)
          BY (instance, cpu)) BY (instance)
        record: instance:node_cpu:ratio
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait"}[5m]))
        record: cluster:node_cpu:sum_rate5m
      - expr: cluster:node_cpu_seconds_total:rate5m / count(sum(node_cpu_seconds_total)
          BY (instance, cpu))
        record: cluster:node_cpu:ratio
    - name: kubernetes-absent
      rules:
      - alert: KubeAPIDown
        annotations:
          description: KubeAPI has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="apiserver"} == 1)
        for: 15m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubeControllerManagerDown
        annotations:
          description: KubeControllerManager has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="kube-controller-manager"} == 1)
        for: 15m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubeSchedulerDown
        annotations:
          description: KubeScheduler has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="kube-scheduler"} == 1)
        for: 15m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubeStateMetricsDown
        annotations:
          description: KubeStateMetrics has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="kube-state-metrics"} == 1)
        for: 15m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubeletDown
        annotations:
          description: Kubelet has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="kubelet"} == 1)
        for: 15m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: NodeExporterDown
        annotations:
          description: NodeExporter has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="monitoring/node_exporter"} == 1)
        for: 15m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: PrometheusDown
        annotations:
          description: Prometheus has disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="monitoring/prometheus"} == 1)
        for: 15m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
    - name: kubernetes-apps
      rules:
      - alert: KubePodCrashLooping
        annotations:
          description: Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
            }}) is restarting {{ printf "%.2f" $value }} times / 5 minutes.
        expr: |
          rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[2m]) * 60 * 5 > 0
        for: 1h
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubePodNotReady
        annotations:
          description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
            state for longer than an hour.
        expr: |
          sum by (namespace, pod, instance, env, idc) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown"}) > 0
        for: 1h
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubePodAtNotReadyStatus
        annotations:
          description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
            state for longer than one minute.
        expr: |
          sum by (namespace, pod, instance, env, idc) (kube_pod_status_ready{job="kube-state-metrics", condition="false"}) > 0
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubeDeploymentReplicasMismatch
        annotations:
          description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not
            matched the expected number of replicas for longer than an hour.
        expr: |
          kube_deployment_spec_replicas{job="kube-state-metrics"}
            !=
          kube_deployment_status_replicas_available{job="kube-state-metrics"}
        for: 1h
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubeStatefulSetReplicasMismatch
        annotations:
          description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has
            not matched the expected number of replicas for longer than 15 minutes.
        expr: |
          kube_statefulset_status_replicas_ready{job="kube-state-metrics"}
            !=
          kube_statefulset_status_replicas{job="kube-state-metrics"}
        for: 15m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubeStatefulSetUpdateNotRolledOut
        annotations:
          description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update
            has not been rolled out.
        expr: |
          max without (revision) (
            kube_statefulset_status_current_revision{job="kube-state-metrics"}
              unless
            kube_statefulset_status_update_revision{job="kube-state-metrics"}
          )
            *
          (
            kube_statefulset_replicas{job="kube-state-metrics"}
              !=
            kube_statefulset_status_replicas_updated{job="kube-state-metrics"}
          )
        for: 15m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubeDaemonSetRolloutStuck
        annotations:
          description: Only {{ $value }}% of the desired Pods of DaemonSet {{ $labels.namespace
            }}/{{ $labels.daemonset }} are scheduled and ready.
        expr: |
          kube_daemonset_status_number_ready{job="kube-state-metrics"}
            /
          kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"} * 100 < 100
        for: 15m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubeDaemonSetNotScheduled
        annotations:
          description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
            }} are not scheduled.'
        expr: |
          kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics"}
            -
          kube_daemonset_status_current_number_scheduled{job="kube-state-metrics"} > 0
        for: 10m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeCronJobRunning
        annotations:
          description: CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more
            than 1h to complete.
        expr: |
          time() - kube_cronjob_next_schedule_time{job="kube-state-metrics"} > 3600
        for: 1h
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeJobCompletion
        annotations:
          description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more
            than one hour to complete.
        expr: |
          kube_job_spec_completions{job="kube-state-metrics"} - kube_job_status_succeeded{job="kube-state-metrics"}  > 0
        for: 1h
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeJobFailed
        annotations:
          description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete.
        expr: |
          kube_job_status_failed{job="kube-state-metrics"}  > 0
        for: 1h
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
    - name: kubernetes-resources
      rules:
      - alert: KubeCPUOvercommit
        annotations:
          description: Cluster has overcommitted CPU resource requests for Pods and cannot
            tolerate node failure.
        expr: |
          sum(namespace:kube_pod_container_resource_requests_cpu_cores:sum)
            /
          sum(kube_node_status_allocatable_cpu_cores)
            >
          (count(kube_node_status_allocatable_cpu_cores)-1) / count(kube_node_status_allocatable_cpu_cores)
        for: 5m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeMemOvercommit
        annotations:
          description: Cluster has overcommitted memory resource requests for Pods and cannot
            tolerate node failure.
        expr: |
          sum(namespace:kube_pod_container_resource_requests_memory_bytes:sum)
            /
          sum(kube_node_status_allocatable_memory_bytes)
            >
          (count(kube_node_status_allocatable_memory_bytes)-1)
            /
          count(kube_node_status_allocatable_memory_bytes)
        for: 5m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeCPUOvercommit
        annotations:
          description: Cluster has overcommitted CPU resource requests for Namespaces.
        expr: |
          sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="cpu"})
            /
          sum(kube_node_status_allocatable_cpu_cores)
            > 1.5
        for: 5m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeMemOvercommit
        annotations:
          description: Cluster has overcommitted memory resource requests for Namespaces.
        expr: |
          sum(kube_resourcequota{job="kube-state-metrics", type="hard", resource="memory"})
            /
          sum(kube_node_status_allocatable_memory_bytes{job="monitoring/node_exporter"})
            > 1.5
        for: 5m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: CPUThrottlingHigh
        annotations:
          description: 'CPU usage in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }} is over {{ printf "%0.0f" $value }}% of limit'
        expr: "100 * sum(increase(container_cpu_cfs_throttled_periods_total{container!=\"\",
          }[5m])) by (container, pod, namespace, idc, env, instance)\n  /\nsum(increase(container_cpu_cfs_periods_total{}[5m]))
          by (container, pod, namespace, idc, env, instance)\n  > 25 \n"
        for: 15m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
    - name: kubernetes-storage
      rules:
      - alert: KubePersistentVolumeUsageCritical
        annotations:
          description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
            }} in Namespace {{ $labels.namespace }} is only {{ printf "%0.2f" $value
            }}% free.
        expr: |
          100 * kubelet_volume_stats_available_bytes{job="kubelet"}
            /
          kubelet_volume_stats_capacity_bytes{job="kubelet"}
            < 3
        for: 1m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubePersistentVolumeFullInFourDays
        annotations:
          description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim
            }} in Namespace {{ $labels.namespace }} is expected to fill up within four
            days. Currently {{ printf "%0.2f" $value }}% is available.
        expr: |
          100 * (
            kubelet_volume_stats_available_bytes{job="kubelet"}
              /
            kubelet_volume_stats_capacity_bytes{job="kubelet"}
          ) < 15
          and
          predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600) < 0
        for: 5m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubePersistentVolumeErrors
        annotations:
          description: The persistent volume {{ $labels.persistentvolume }} has status {{
            $labels.phase }}.
        expr: |
          kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
        for: 5m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
    - name: kubernetes-system
      rules:
      - alert: KubeNodeNotReady
        annotations:
          description: '{{ $labels.node }} has been unready for more than an hour.'
        expr: |
          kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
        for: 1h
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeVersionMismatch
        annotations:
          description: There are {{ $value }} different semantic versions of Kubernetes
            components running.
        expr: |
          count(count by (gitVersion) (label_replace(kubernetes_build_info{job!="kube-dns"},"gitVersion","$1","gitVersion","(v[0-9]*.[0-9]*.[0-9]*).*"))) > 1
        for: 1h
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeClientErrors
        annotations:
          description: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
            }}' is experiencing {{ printf "%0.0f" $value }}% errors.'
        expr: |
          (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (instance, job)
            /
          sum(rate(rest_client_requests_total[5m])) by (instance, job))
          * 100 > 1
        for: 15m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeClientErrors
        annotations:
          description: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ printf "%0.0f" $value }} errors / second.
        expr: |
          sum(rate(ksm_scrape_error_total{job="kube-state-metrics"}[5m])) by (instance, job) > 0.1
        for: 15m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeAPILatencyHigh
        annotations:
          description: The API server has a 99th percentile latency of {{ $value }} seconds
            for {{ $labels.verb }} {{ $labels.resource }}.
        expr: |
          cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 1
        for: 10m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeAPILatencyHigh
        annotations:
          description: The API server has a 99th percentile latency of {{ $value }} seconds
            for {{ $labels.verb }} {{ $labels.resource }}.
        expr: |
          cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{job="apiserver",quantile="0.99",subresource!="log",verb!~"^(?:LIST|WATCH|WATCHLIST|PROXY|CONNECT)$"} > 4
        for: 10m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubeAPIErrorsHigh
        annotations:
          description: API server is returning errors for {{ $value }}% of requests.
        expr: |
          sum(rate(apiserver_request_total{job="apiserver",code=~"^(?:5..)$"}[5m]))
            /
          sum(rate(apiserver_request_total{job="apiserver"}[5m])) * 100 > 3
        for: 10m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubeAPIErrorsHigh
        annotations:
          description: API server is returning errors for {{ $value }}% of requests.
        expr: |
          sum(rate(apiserver_request_total{job="apiserver",code=~"^(?:5..)$"}[5m]))
            /
          sum(rate(apiserver_request_total{job="apiserver"}[5m])) * 100 > 1
        for: 10m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeAPIErrorsHigh
        annotations:
          description: API server is returning errors for {{ $value }}% of requests for
            {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}.
        expr: |
          sum(rate(apiserver_request_total{job="apiserver",code=~"^(?:5..)$"}[5m])) by (resource,subresource,verb)
            /
          sum(rate(apiserver_request_total{job="apiserver"}[5m])) by (resource,subresource,verb) * 100 > 10
        for: 10m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
      - alert: KubeAPIErrorsHigh
        annotations:
          description: API server is returning errors for {{ $value }}% of requests for
            {{ $labels.verb }} {{ $labels.resource }} {{ $labels.subresource }}.
        expr: |
          sum(rate(apiserver_request_total{job="apiserver",code=~"^(?:5..)$"}[5m])) by (resource,subresource,verb)
            /
          sum(rate(apiserver_request_total{job="apiserver"}[5m])) by (resource,subresource,verb) * 100 > 5
        for: 10m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeClientCertificateExpiration
        annotations:
          description: A client certificate used to authenticate to the apiserver is expiring
            in less than 7.0 days.
        expr: |
          apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 604800
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: KubeClientCertificateExpiration
        annotations:
          description: A client certificate used to authenticate to the apiserver is expiring
            in less than 24.0 hours.
        expr: |
          apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 86400
        labels:
          severity: Error
          env: "{{ $labels.env }}"
    - name: general.rules
      rules:
    - name: kube-prometheus-node-alerting.rules
      rules:
      - alert: DiskRunningFull
        annotations:
          description: Device {{ $labels.device }} of node-exporter {{ $labels.namespace
            }}/{{ $labels.pod }} will be full within the next 24 hours.
        expr: |
          (node:node_filesystem_usage: > 0.85) and (predict_linear(node:node_filesystem_avail:[6h], 3600 * 24) < 0)
        for: 30m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: DiskRunningFull
        annotations:
          description: Device {{ $labels.device }} of node-exporter {{ $labels.namespace
            }}/{{ $labels.pod }} will be full within the next 2 hours.
        expr: |
          (node:node_filesystem_usage: > 0.85) and (predict_linear(node:node_filesystem_avail:[30m], 3600 * 2) < 0)
        for: 10m
        labels:
          severity: Error
          env: "{{ $labels.env }}"
    - name: node-network
      rules:
      - alert: NetworkReceiveErrors
        annotations:
          description: Network interface "{{ $labels.device }}" showing receive errors on
            node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
        expr: |
          rate(node_network_receive_errs_total{job="monitoring/node_exporter",device!~"veth.+"}[2m]) > 0
        for: 2m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: NetworkTransmitErrors
        annotations:
          description: Network interface "{{ $labels.device }}" showing transmit errors
            on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
        expr: |
          rate(node_network_transmit_errs_total{job="monitoring/node_exporter",device!~"veth.+"}[2m]) > 0
        for: 2m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
      - alert: NodeNetworkInterfaceFlapping
        annotations:
          description: Network interface "{{ $labels.device }}" changing it's up status
            often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}"
        expr: |
          changes(node_network_up{job="monitoring/node_exporter",device!~"veth.+"}[2m]) > 2
        for: 2m
        labels:
          severity: Warning
          env: "{{ $labels.env }}"
